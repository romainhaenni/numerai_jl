# Example configuration file demonstrating neural network integration
# Copy to config.toml and modify as needed

# API credentials (can also be set via environment variables)
# api_public_key = "your_public_key"
# api_secret_key = "your_secret_key"

# Model configuration
models = [
    # Traditional ensemble models
    "xgb_deep",
    "xgb_shallow", 
    "lgbm_small",
    "lgbm_large",
    
    # Neural network models (NEW!)
    "mlp_default",      # Multi-Layer Perceptron
    "resnet_small",     # ResNet with skip connections
    "tabnet_medium"     # TabNet for tabular data
]

# Data and output directories
data_dir = "data"
model_dir = "models"

# Tournament settings
auto_submit = true
stake_amount = 10.0
max_workers = 8
notification_enabled = true

# Compounding settings
compounding_enabled = false
min_compound_amount = 1.0
compound_percentage = 100.0
max_stake_amount = 10000.0

# Neural network specific configuration examples:
# Note: These would typically be set programmatically via ModelConfig

[neural_networks]
# Enable/disable GPU acceleration for neural networks
gpu_enabled = true

# Default training epochs for quick testing
default_epochs = 50

# Example MLP configuration
[neural_networks.mlp]
hidden_layers = [256, 128, 64, 32]
dropout_rate = 0.2
learning_rate = 0.001
batch_size = 512
epochs = 100
early_stopping_patience = 10

# Example ResNet configuration  
[neural_networks.resnet]
hidden_layers = [256, 256, 256, 128]
dropout_rate = 0.1
learning_rate = 0.001
batch_size = 512
epochs = 150
early_stopping_patience = 15

# Example TabNet configuration
[neural_networks.tabnet]
n_d = 64                    # Decision layer dimension
n_a = 64                    # Attention layer dimension
n_steps = 3                 # Number of steps
gamma = 1.3                 # Feature reusage coefficient
learning_rate = 0.02
batch_size = 1024
epochs = 200
early_stopping_patience = 20