        - module Models
        - 
        - using XGBoost
        - using LightGBM
        - using EvoTrees
        - using DataFrames
        - using Random
        - using Statistics
        - using ThreadsX
        - using OrderedCollections
        - 
        - abstract type NumeraiModel end
        - 
        - mutable struct XGBoostModel <: NumeraiModel
        6     model::Union{Nothing, Booster}
        -     params::Dict{String, Any}
        -     num_rounds::Int
        -     name::String
        - end
        - 
        - mutable struct LightGBMModel <: NumeraiModel
        6     model::Union{Nothing, LGBMRegression}
        -     params::Dict{String, Any}
        -     name::String
        - end
        - 
        - mutable struct EvoTreesModel <: NumeraiModel
        2     model::Union{Nothing, EvoTrees.EvoTree}
        -     params::Dict{String, Any}
        -     name::String
        - end
        - 
       12 function XGBoostModel(name::String="xgboost_default"; 
        -                      max_depth::Int=5,
        -                      learning_rate::Float64=0.01,
        -                      colsample_bytree::Float64=0.1,
        -                      num_rounds::Int=1000)
        6     params = Dict(
        -         "max_depth" => max_depth,
        -         "learning_rate" => learning_rate,
        -         "colsample_bytree" => colsample_bytree,
        -         "objective" => "reg:squarederror",
        -         "eval_metric" => "rmse",
        -         "tree_method" => "hist",
        -         "device" => "cpu",
        -         "nthread" => Threads.nthreads()
        -     )
        -     
        6     return XGBoostModel(nothing, params, num_rounds, name)
        - end
        - 
       12 function LightGBMModel(name::String="lgbm_default";
        -                       num_leaves::Int=31,
        -                       learning_rate::Float64=0.01,
        -                       feature_fraction::Float64=0.1,
        -                       n_estimators::Int=1000)
        6     params = Dict(
        -         "objective" => "regression",
        -         "metric" => "rmse",
        -         "num_leaves" => num_leaves,
        -         "learning_rate" => learning_rate,
        -         "feature_fraction" => feature_fraction,
        -         "bagging_fraction" => 0.8,
        -         "bagging_freq" => 5,
        -         "n_estimators" => n_estimators,
        -         "num_threads" => Threads.nthreads(),
        -         "verbose" => -1
        -     )
        -     
        6     return LightGBMModel(nothing, params, name)
        - end
        - 
        4 function EvoTreesModel(name::String="evotrees_default";
        -                       max_depth::Int=5,
        -                       learning_rate::Float64=0.01,
        -                       subsample::Float64=0.8,
        -                       colsample::Float64=0.8,
        -                       nrounds::Int=1000)
        2     params = Dict(
        -         "loss" => :mse,
        -         "metric" => :mse,
        -         "max_depth" => max_depth,
        -         "eta" => learning_rate,
        -         "rowsample" => subsample,
        -         "colsample" => colsample,
        -         "nrounds" => nrounds,
        -         "nbins" => 64,
        -         "monotone_constraints" => Dict{Int, Int}(),
        -         "device" => "cpu"
        -     )
        -     
        2     return EvoTreesModel(nothing, params, name)
        - end
        - 
        8 function train!(model::XGBoostModel, X_train::Matrix{Float64}, y_train::Vector{Float64};
        -                X_val::Union{Nothing, Matrix{Float64}}=nothing,
        -                y_val::Union{Nothing, Vector{Float64}}=nothing,
        -                verbose::Bool=false)
        -     
        4     dtrain = DMatrix(X_train, label=y_train)
        -     
        -     # Train model with individual parameters instead of params dict
        4     if X_val !== nothing && y_val !== nothing
        2         dval = DMatrix(X_val, label=y_val)
        2         watchlist = OrderedDict("train" => dtrain, "eval" => dval)
        -         
        -         # Train model with validation set
        2         model.model = xgboost(
        -             dtrain;
        -             num_round=model.num_rounds,
        -             max_depth=model.params["max_depth"],
        -             eta=model.params["learning_rate"],
        -             colsample_bytree=model.params["colsample_bytree"],
        -             objective=model.params["objective"],
        -             eval_metric=model.params["eval_metric"],
        -             tree_method=model.params["tree_method"],
        -             device=model.params["device"],
        -             nthread=model.params["nthread"],
        -             watchlist=watchlist
        -         )
        -     else
        -         # Train model without validation set
        2         model.model = xgboost(
        -             dtrain;
        -             num_round=model.num_rounds,
        -             max_depth=model.params["max_depth"],
        -             eta=model.params["learning_rate"],
        -             colsample_bytree=model.params["colsample_bytree"],
        -             objective=model.params["objective"],
        -             eval_metric=model.params["eval_metric"],
        -             tree_method=model.params["tree_method"],
        -             device=model.params["device"],
        -             nthread=model.params["nthread"]
        -         )
        -     end
        -     
        4     return model
        - end
        - 
        8 function train!(model::LightGBMModel, X_train::Matrix{Float64}, y_train::Vector{Float64};
        -                X_val::Union{Nothing, Matrix{Float64}}=nothing,
        -                y_val::Union{Nothing, Vector{Float64}}=nothing,
        -                verbose::Bool=false)
        -     
        4     estimator = LGBMRegression(;
        -         objective=model.params["objective"],
        -         metric=[model.params["metric"]],
        -         num_leaves=model.params["num_leaves"],
        -         learning_rate=model.params["learning_rate"],
        -         feature_fraction=model.params["feature_fraction"],
        -         bagging_fraction=model.params["bagging_fraction"],
        -         bagging_freq=model.params["bagging_freq"],
        -         num_iterations=model.params["n_estimators"],
        -         num_threads=model.params["num_threads"],
        -         verbosity=verbose ? 1 : -1
        -     )
        -     
        -     # Use the correct parameter names for LightGBM.jl v2.0.0
        4     if X_val !== nothing && y_val !== nothing
        2         LightGBM.fit!(estimator, X_train, y_train, (X_val, y_val);
        -                      verbosity=verbose ? 1 : -1,
        -                      is_row_major=false,
        -                      weights=Float32[],
        -                      init_score=Float64[],
        -                      group=Int64[],
        -                      truncate_booster=false)
        -     else
        2         LightGBM.fit!(estimator, X_train, y_train;
        -                      verbosity=verbose ? 1 : -1,
        -                      is_row_major=false,
        -                      weights=Float32[],
        -                      init_score=Float64[],
        -                      group=Int64[],
        -                      truncate_booster=false)
        -     end
        -     
        4     model.model = estimator
        -     
        4     return model
        - end
        - 
        0 function train!(model::EvoTreesModel, X_train::Matrix{Float64}, y_train::Vector{Float64};
        -                X_val::Union{Nothing, Matrix{Float64}}=nothing,
        -                y_val::Union{Nothing, Vector{Float64}}=nothing,
        -                verbose::Bool=false)
        -     
        -     # Prepare configuration for EvoTrees
        0     config = EvoTrees.EvoTreeRegressor(;
        -         loss=model.params["loss"],
        -         metric=model.params["metric"],
        -         max_depth=model.params["max_depth"],
        -         eta=model.params["eta"],
        -         rowsample=model.params["rowsample"],
        -         colsample=model.params["colsample"],
        -         nrounds=model.params["nrounds"],
        -         nbins=model.params["nbins"],
        -         monotone_constraints=model.params["monotone_constraints"],
        -         device=model.params["device"]
        -     )
        -     
        -     # Train the model
        0     if X_val !== nothing && y_val !== nothing
        -         # Train with validation set for early stopping
        0         model.model = EvoTrees.fit_evotree(config; 
        -                                           x_train=X_train, 
        -                                           y_train=y_train,
        -                                           x_eval=X_val,
        -                                           y_eval=y_val,
        -                                           print_every_n=verbose ? 100 : 0,
        -                                           early_stopping_rounds=10)
        -     else
        -         # Train without validation set
        0         model.model = EvoTrees.fit_evotree(config; 
        -                                           x_train=X_train, 
        -                                           y_train=y_train,
        -                                           print_every_n=verbose ? 100 : 0)
        -     end
        -     
        0     return model
        - end
        - 
        4 function predict(model::XGBoostModel, X::Matrix{Float64})::Vector{Float64}
        4     if model.model === nothing
        0         error("Model not trained yet")
        -     end
        -     
        4     dtest = DMatrix(X)
        8     predictions = XGBoost.predict(model.model, dtest)
        -     
        4     return predictions
        - end
        - 
        4 function predict(model::LightGBMModel, X::Matrix{Float64})::Vector{Float64}
        4     if model.model === nothing
        0         error("Model not trained yet")
        -     end
        -     
        4     predictions = LightGBM.predict(model.model, X)
        -     
        -     # Convert to Vector if it's a Matrix (LightGBM.jl v2.0.0 sometimes returns Matrix)
        4     if predictions isa Matrix
        4         return vec(predictions)
        -     else
        0         return predictions
        -     end
        - end
        - 
        0 function predict(model::EvoTreesModel, X::Matrix{Float64})::Vector{Float64}
        0     if model.model === nothing
        0         error("Model not trained yet")
        -     end
        -     
        0     predictions = EvoTrees.predict(model.model, X)
        -     
        -     # Convert to Vector if it's a Matrix
        0     if predictions isa Matrix
        0         return vec(predictions)
        -     else
        0         return predictions
        -     end
        - end
        - 
        - function cross_validate(model_constructor::Function, X::Matrix{Float64}, y::Vector{Float64}, 
        -                        eras::Vector{Int}; n_splits::Int=5)::Vector{Float64}
        -     unique_eras = unique(eras)
        -     n_eras = length(unique_eras)
        -     era_size = n_eras ÷ n_splits
        -     
        -     cv_scores = Float64[]
        -     
        -     for i in 1:n_splits
        -         val_start = (i - 1) * era_size + 1
        -         val_end = min(i * era_size, n_eras)
        -         val_eras = unique_eras[val_start:val_end]
        -         
        -         train_mask = .!(in.(eras, Ref(val_eras)))
        -         val_mask = in.(eras, Ref(val_eras))
        -         
        -         X_train = X[train_mask, :]
        -         y_train = y[train_mask]
        -         X_val = X[val_mask, :]
        -         y_val = y[val_mask]
        -         
        -         model = model_constructor()
        -         train!(model, X_train, y_train)
        -         
        -         predictions = predict(model, X_val)
        -         score = cor(predictions, y_val)
        -         push!(cv_scores, score)
        -     end
        -     
        -     return cv_scores
        - end
        - 
        - function feature_importance(model::XGBoostModel)::Dict{String, Float64}
        -     if model.model === nothing
        -         error("Model not trained yet")
        -     end
        -     
        -     importance = XGBoost.importance(model.model)
        -     return importance
        - end
        - 
        - function feature_importance(model::LightGBMModel)::Dict{String, Float64}
        -     if model.model === nothing
        -         error("Model not trained yet")
        -     end
        -     
        -     importance = LightGBM.feature_importance(model.model)
        -     feature_names = LightGBM.feature_name(model.model)
        -     
        -     return Dict(zip(feature_names, importance))
        - end
        - 
        - function feature_importance(model::EvoTreesModel)::Dict{String, Float64}
        -     if model.model === nothing
        -         error("Model not trained yet")
        -     end
        -     
        -     importance = EvoTrees.importance(model.model)
        -     
        -     # Convert importance to dictionary with feature names
        -     feature_dict = Dict{String, Float64}()
        -     for i in 1:length(importance)
        -         feature_dict["feature_$(i)"] = importance[i]
        -     end
        -     
        -     return feature_dict
        - end
        - 
        - function save_model(model::NumeraiModel, filepath::String)
        -     if model.model === nothing
        -         error("Model not trained yet")
        -     end
        -     
        -     if model isa XGBoostModel
        -         XGBoost.save(model.model, filepath)
        -     elseif model isa LightGBMModel
        -         LightGBM.savemodel(model.model, filepath)
        -     elseif model isa EvoTreesModel
        -         EvoTrees.save(model.model, filepath)
        -     end
        -     
        -     println("Model saved to $filepath")
        - end
        - 
        - function load_model!(model::XGBoostModel, filepath::String)
        -     model.model = Booster(model_file=filepath)
        -     return model
        - end
        - 
        - function load_model!(model::LightGBMModel, filepath::String)
        -     model.model = LightGBM.loadmodel(filepath)
        -     return model
        - end
        - 
        - function load_model!(model::EvoTreesModel, filepath::String)
        -     model.model = EvoTrees.load(filepath)
        -     return model
        - end
        - 
        - export NumeraiModel, XGBoostModel, LightGBMModel, EvoTreesModel, train!, predict, 
        -        cross_validate, feature_importance, save_model, load_model!
        - 
        - end
