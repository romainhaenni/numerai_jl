        - module HyperOpt
        - 
        - using ..Logger
        - using ..Models
        - using ..Metrics
        - using DataFrames
        - using Statistics: mean, std, cor
        - using Random
        - using Distributed
        - using ProgressMeter
        - using JSON3
        - using Dates: now
        - using LinearAlgebra: dot, cholesky, Symmetric, I, norm
        - using Distributions: Normal, cdf, pdf
        - 
        - export HyperOptConfig, GridSearchOptimizer, RandomSearchOptimizer, BayesianOptimizer,
        -        optimize_hyperparameters, evaluate_params, get_best_params,
        -        create_param_grid, create_param_distributions, OptimizationResult,
        -        calculate_expected_improvement, calculate_upper_confidence_bound
        - 
        - # Helper function for time series data splitting
        - function split_time_series_data(
        -     data::Dict{String,DataFrame},
        -     split_idx::Int,
        -     n_splits::Int,
        -     validation_eras::Vector{Int}
        - )
        -     train_data = Dict{String,DataFrame}()
        -     val_data = Dict{String,DataFrame}()
        -     
        -     for (target, df) in data
        -         if "era" in names(df)
        -             # Time series split based on eras
        -             unique_eras = sort(unique(df.era))
        -             n_eras = length(unique_eras)
        -             
        -             # Calculate split points
        -             val_size = n_eras ÷ n_splits
        -             val_start_idx = (split_idx - 1) * val_size + 1
        -             val_end_idx = min(split_idx * val_size, n_eras)
        -             
        -             # Get validation eras
        -             if !isempty(validation_eras)
        -                 val_eras = validation_eras
        -             else
        -                 val_eras = unique_eras[val_start_idx:val_end_idx]
        -             end
        -             
        -             # Split data
        -             train_mask = .!(df.era .∈ Ref(val_eras))
        -             val_mask = df.era .∈ Ref(val_eras)
        -             
        -             train_data[target] = df[train_mask, :]
        -             val_data[target] = df[val_mask, :]
        -         else
        -             # Simple percentage split if no era column
        -             n_rows = nrow(df)
        -             val_size = n_rows ÷ n_splits
        -             val_start = (split_idx - 1) * val_size + 1
        -             val_end = min(split_idx * val_size, n_rows)
        -             
        -             train_indices = vcat(1:val_start-1, val_end+1:n_rows)
        -             val_indices = val_start:val_end
        -             
        -             train_data[target] = df[train_indices, :]
        -             val_data[target] = df[val_indices, :]
        -         end
        -     end
        -     
        -     return train_data, val_data
        - end
        - 
        - # Optimization result structure
        - struct OptimizationResult
        3     best_params::Dict{Symbol,Any}
        -     best_score::Float64
        -     all_results::DataFrame
        -     optimization_history::Vector{Dict{Symbol,Any}}
        -     cross_validation_scores::Vector{Float64}
        -     training_time::Float64
        - end
        - 
        - # Configuration for hyperparameter optimization
        - struct HyperOptConfig
        -     model_type::Symbol
        -     objective::Symbol  # :correlation, :sharpe, :mmc, :tc, :multi_objective
        -     n_splits::Int  # Number of cross-validation splits
        -     validation_eras::Vector{Int}  # Eras to use for validation
        -     early_stopping_rounds::Int
        -     verbose::Bool
        -     parallel::Bool  # Use parallel processing
        -     seed::Int
        -     correlation_weight::Float64  # Weight for correlation in multi-objective
        -     sharpe_weight::Float64  # Weight for sharpe in multi-objective
        -     
       10     function HyperOptConfig(;
        -         model_type::Symbol,
        -         objective::Symbol=:correlation,
        -         n_splits::Int=3,
        -         validation_eras::Vector{Int}=Int[],
        -         early_stopping_rounds::Int=10,
        -         verbose::Bool=true,
        -         parallel::Bool=true,
        -         seed::Int=42,
        -         correlation_weight::Float64=0.7,
        -         sharpe_weight::Float64=0.3
        -     )
        5         new(model_type, objective, n_splits, validation_eras, 
        -             early_stopping_rounds, verbose, parallel, seed,
        -             correlation_weight, sharpe_weight)
        -     end
        - end
        - 
        - # Abstract type for all optimizers
        - abstract type HyperOptimizer end
        - 
        - # Grid Search Optimizer
        - struct GridSearchOptimizer <: HyperOptimizer
        2     param_grid::Dict{Symbol,Vector}
        -     config::HyperOptConfig
        - end
        - 
        - # Random Search Optimizer
        - struct RandomSearchOptimizer <: HyperOptimizer
        1     param_distributions::Dict{Symbol,Any}
        -     n_iter::Int
        -     config::HyperOptConfig
        - end
        - 
        - # Bayesian Optimizer using Gaussian Processes
        - struct BayesianOptimizer <: HyperOptimizer
        1     param_bounds::Dict{Symbol,Tuple{Float64,Float64}}
        -     n_initial::Int  # Number of random initial points
        -     n_iter::Int  # Number of optimization iterations
        -     acquisition_function::Symbol  # :ei (expected improvement), :ucb (upper confidence bound)
        -     config::HyperOptConfig
        - end
        - 
        - # Create parameter grid for different model types
        6 function create_param_grid(model_type::Symbol)
        6     if model_type == :XGBoost
        1         return Dict(
        -             :max_depth => [3, 5, 7, 10],
        -             :learning_rate => [0.001, 0.01, 0.05, 0.1],
        -             :n_estimators => [100, 200, 500, 1000],
        -             :colsample_bytree => [0.1, 0.3, 0.5, 0.7],
        -             :subsample => [0.5, 0.7, 0.9, 1.0],
        -             :min_child_weight => [1, 3, 5],
        -             :gamma => [0, 0.1, 0.3, 0.5],
        -             :reg_alpha => [0, 0.001, 0.01, 0.1],
        -             :reg_lambda => [0, 0.001, 0.01, 0.1]
        -         )
        5     elseif model_type == :LightGBM
        1         return Dict(
        -             :num_leaves => [15, 31, 63, 127],
        -             :learning_rate => [0.001, 0.01, 0.05, 0.1],
        -             :n_estimators => [100, 200, 500, 1000],
        -             :feature_fraction => [0.1, 0.3, 0.5, 0.7],
        -             :bagging_fraction => [0.5, 0.7, 0.9, 1.0],
        -             :bagging_freq => [0, 1, 5],
        -             :min_data_in_leaf => [10, 20, 50, 100],
        -             :lambda_l1 => [0, 0.001, 0.01, 0.1],
        -             :lambda_l2 => [0, 0.001, 0.01, 0.1]
        -         )
        4     elseif model_type == :EvoTrees
        0         return Dict(
        -             :max_depth => [3, 5, 7, 10],
        -             :eta => [0.001, 0.01, 0.05, 0.1],
        -             :nrounds => [100, 200, 500, 1000],
        -             :subsample => [0.5, 0.7, 0.9, 1.0],
        -             :colsample => [0.1, 0.3, 0.5, 0.7],
        -             :gamma => [0, 0.1, 0.3, 0.5],
        -             :lambda => [0, 0.001, 0.01, 0.1],
        -             :alpha => [0, 0.001, 0.01, 0.1]
        -         )
        4     elseif model_type == :CatBoost
        0         return Dict(
        -             :depth => [3, 5, 7, 10],
        -             :learning_rate => [0.001, 0.01, 0.05, 0.1],
        -             :iterations => [100, 200, 500, 1000],
        -             :l2_leaf_reg => [1, 3, 5, 10],
        -             :bagging_temperature => [0, 0.5, 1.0],
        -             :random_strength => [0, 0.5, 1.0, 2.0],
        -             :border_count => [32, 64, 128, 254]
        -         )
        4     elseif model_type == :Ridge
        1         return Dict(
        -             :alpha => [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
        -         )
        3     elseif model_type == :Lasso
        1         return Dict(
        -             :alpha => [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
        -         )
        2     elseif model_type == :ElasticNet
        1         return Dict(
        -             :alpha => [0.001, 0.01, 0.1, 1.0, 10.0],
        -             :l1_ratio => [0.1, 0.3, 0.5, 0.7, 0.9]
        -         )
        1     elseif model_type == :NeuralNetwork
        1         return Dict(
        -             :hidden_layers => [[128, 64], [256, 128, 64], [512, 256, 128], [128, 64, 32]],
        -             :learning_rate => [0.0001, 0.001, 0.01, 0.1],
        -             :batch_size => [256, 512, 1024, 2048],
        -             :epochs => [10, 20, 50, 100],
        -             :dropout_rate => [0.0, 0.1, 0.2, 0.3],
        -             :activation => [:relu, :tanh, :sigmoid]
        -         )
        -     else
        0         error("Unknown model type: $model_type")
        -     end
        - end
        - 
        - # Create parameter distributions for random search
        2 function create_param_distributions(model_type::Symbol)
        2     if model_type == :XGBoost
        1         return Dict(
        1             :max_depth => () -> rand(3:15),
        1             :learning_rate => () -> 10.0^(rand(-4:-1)),
        -             :n_estimators => () -> rand(100:2000),
        -             :colsample_bytree => () -> rand(0.1:0.1:1.0),
        -             :subsample => () -> rand(0.5:0.1:1.0),
        -             :min_child_weight => () -> rand(1:10),
        -             :gamma => () -> rand(0:0.1:1.0),
        -             :reg_alpha => () -> 10.0^(rand(-4:0)),
        -             :reg_lambda => () -> 10.0^(rand(-4:0))
        -         )
        1     elseif model_type == :LightGBM
        0         return Dict(
        -             :num_leaves => () -> rand(10:200),
        -             :learning_rate => () -> 10.0^(rand(-4:-1)),
        -             :n_estimators => () -> rand(100:2000),
        -             :feature_fraction => () -> rand(0.1:0.1:1.0),
        -             :bagging_fraction => () -> rand(0.5:0.1:1.0),
        -             :bagging_freq => () -> rand(0:10),
        -             :min_data_in_leaf => () -> rand(5:200),
        -             :lambda_l1 => () -> 10.0^(rand(-4:0)),
        -             :lambda_l2 => () -> 10.0^(rand(-4:0))
        -         )
        1     elseif model_type == :EvoTrees
        0         return Dict(
        -             :max_depth => () -> rand(3:15),
        -             :eta => () -> 10.0^(rand(-4:-1)),
        -             :nrounds => () -> rand(100:2000),
        -             :subsample => () -> rand(0.5:0.1:1.0),
        -             :colsample => () -> rand(0.1:0.1:1.0),
        -             :gamma => () -> rand(0:0.1:1.0),
        -             :lambda => () -> 10.0^(rand(-4:0)),
        -             :alpha => () -> 10.0^(rand(-4:0))
        -         )
        1     elseif model_type == :CatBoost
        0         return Dict(
        -             :depth => () -> rand(3:12),
        -             :learning_rate => () -> 10.0^(rand(-4:-1)),
        -             :iterations => () -> rand(100:2000),
        -             :l2_leaf_reg => () -> rand(1:30),
        -             :bagging_temperature => () -> rand(0:0.1:2.0),
        -             :random_strength => () -> rand(0:0.1:3.0),
        -             :border_count => () -> rand([32, 64, 128, 254])
        -         )
        1     elseif model_type == :Ridge
        0         return Dict(
        -             :alpha => () -> 10.0^(rand(-3:3))
        -         )
        1     elseif model_type == :Lasso
        0         return Dict(
        -             :alpha => () -> 10.0^(rand(-4:2))
        -         )
        1     elseif model_type == :ElasticNet
        0         return Dict(
        -             :alpha => () -> 10.0^(rand(-3:2)),
        -             :l1_ratio => () -> rand()
        -         )
        1     elseif model_type == :NeuralNetwork
        1         return Dict(
        1             :hidden_layers => () -> begin
        1                 n_layers = rand(2:4)
        1                 layers = Int[]
        1                 prev_size = 512
        1                 for i in 1:n_layers
        3                     size = rand(32:prev_size)
        3                     push!(layers, size)
        3                     prev_size = size
        5                 end
        1                 layers
        -             end,
        -             :learning_rate => () -> 10.0^(rand(-4:-1)),
        -             :batch_size => () -> 2^rand(8:11),
        -             :epochs => () -> rand(10:100),
        -             :dropout_rate => () -> rand(0:0.05:0.5),
        -             :activation => () -> rand([:relu, :tanh, :sigmoid])
        -         )
        -     else
        0         error("Unknown model type: $model_type")
        -     end
        - end
        - 
        - # Evaluate parameters using cross-validation
        - function evaluate_params(
        -     params::Dict{Symbol,Any},
        -     data::Dict{String,DataFrame},
        -     config::HyperOptConfig,
        -     targets::Vector{String}
        - )
        -     try
        -         scores = Float64[]
        -         
        -         # Perform cross-validation
        -         for split_idx in 1:config.n_splits
        -             # Split data by eras for time series cross-validation
        -             # Simple time-series split implementation
        -             train_data, val_data = split_time_series_data(
        -                 data, 
        -                 split_idx, 
        -                 config.n_splits,
        -                 config.validation_eras
        -             )
        -             
        -             # Create and train model
        -             model = Models.create_model(config.model_type, params)
        -             Models.train!(model, train_data)
        -             
        -             # Make predictions
        -             predictions = Models.predict(model, val_data)
        -             
        -             # Calculate objective score
        -             score = calculate_objective_score(
        -                 predictions,
        -                 val_data,
        -                 config.objective,
        -                 targets,
        -                 config
        -             )
        -             
        -             push!(scores, score)
        -         end
        -         
        -         # Return mean score across folds
        -         return mean(scores), scores
        -         
        -     catch e
        -         Logger.@log_error "Error evaluating parameters: $e"
        -         return -Inf, Float64[]  # Return worst possible score on error
        -     end
        - end
        - 
        - # Calculate objective score based on specified metric
       10 function calculate_objective_score(
        -     predictions::DataFrame,
        -     val_data::Dict{String,DataFrame},
        -     objective::Symbol,
        -     targets::Vector{String},
        -     config::Union{HyperOptConfig, Nothing}=nothing
        - )
       10     if objective == :correlation
        -         # Average correlation across all targets
        2         correlations = Float64[]
        2         for target in targets
        6             if haskey(val_data, target) && target in names(predictions)
        4                 corr = cor(
        -                     predictions[!, target],
        -                     val_data[target][!, "target"]
        -                 )
        4                 push!(correlations, corr)
        -             end
        4         end
        2         return isempty(correlations) ? -Inf : mean(correlations)
        -         
        3     elseif objective == :sharpe
        -         # Calculate Sharpe ratio
        2         returns = Float64[]
        2         for target in targets
        6             if haskey(val_data, target) && target in names(predictions)
        4                 corr = cor(
        -                     predictions[!, target],
        -                     val_data[target][!, "target"]
        -                 )
        4                 push!(returns, corr)
        -             end
        4         end
        2         return isempty(returns) ? -Inf : Metrics.calculate_sharpe(returns)
        -         
        1     elseif objective == :mmc
        -         # Calculate Meta Model Contribution
        0         mmc_scores = Float64[]
        0         for target in targets
        0             if haskey(val_data, target) && target in names(predictions)
        -                 # Get era information
        0                 eras = unique(val_data[target][!, "era"])
        0                 era_mmc = Float64[]
        -                 
        0                 for era in eras
        0                     era_mask = val_data[target][!, "era"] .== era
        0                     era_preds = predictions[era_mask, target]
        0                     era_targets = val_data[target][era_mask, "target"]
        -                     
        -                     # Calculate MMC (requires meta model predictions)
        -                     # For now, use correlation as proxy if meta model not available
        0                     mmc = Metrics.calculate_mmc(era_preds, era_targets)
        0                     push!(era_mmc, mmc)
        0                 end
        -                 
        0                 push!(mmc_scores, mean(era_mmc))
        -             end
        0         end
        0         return isempty(mmc_scores) ? -Inf : mean(mmc_scores)
        -         
        1     elseif objective == :tc
        -         # Calculate True Contribution
        0         tc_scores = Float64[]
        0         for target in targets
        0             if haskey(val_data, target) && target in names(predictions)
        0                 tc = Metrics.calculate_tc(
        -                     predictions[!, target],
        -                     val_data[target][!, "target"]
        -                 )
        0                 push!(tc_scores, tc)
        -             end
        0         end
        0         return isempty(tc_scores) ? -Inf : mean(tc_scores)
        -         
        1     elseif objective == :multi_objective
        -         # Weighted combination of multiple objectives
        1         corr_score = calculate_objective_score(predictions, val_data, :correlation, targets)
        1         sharpe_score = calculate_objective_score(predictions, val_data, :sharpe, targets)
        -         
        -         # Use weights from config if available, otherwise use defaults
        1         corr_weight = config !== nothing ? config.correlation_weight : 0.7
        1         sharpe_weight = config !== nothing ? config.sharpe_weight : 0.3
        -         
        1         return corr_weight * corr_score + sharpe_weight * sharpe_score
        -         
        -     else
        0         error("Unknown objective: $objective")
        -     end
        - end
        - 
        - # Grid Search implementation
        - function optimize_hyperparameters(
        -     optimizer::GridSearchOptimizer,
        -     data::Dict{String,DataFrame},
        -     targets::Vector{String}
        - )
        -     start_time = time()
        -     
        -     # Generate all parameter combinations
        -     param_names = collect(keys(optimizer.param_grid))
        -     param_values = [optimizer.param_grid[name] for name in param_names]
        -     param_combinations = vec(collect(Iterators.product(param_values...)))
        -     
        -     n_combinations = length(param_combinations)
        -     Logger.@log_info("Starting Grid Search with $n_combinations parameter combinations")
        -     
        -     # Storage for results
        -     all_results = DataFrame()
        -     optimization_history = Vector{Dict{Symbol,Any}}()
        -     best_score = -Inf
        -     best_params = Dict{Symbol,Any}()
        -     best_cv_scores = Float64[]
        -     
        -     # Progress meter
        -     progress = Progress(n_combinations, desc="Grid Search: ", showspeed=true)
        -     
        -     # Evaluate each combination
        -     if optimizer.config.parallel && nworkers() > 1
        -         # Parallel evaluation
        -         results = pmap(param_combinations) do combination
        -             params = Dict(zip(param_names, combination))
        -             score, cv_scores = evaluate_params(params, data, optimizer.config, targets)
        -             return (params=params, score=score, cv_scores=cv_scores)
        -         end
        -         
        -         for result in results
        -             best_score = update_best_params!(
        -                 result.params, result.score, result.cv_scores,
        -                 best_params, best_score, best_cv_scores,
        -                 optimization_history, all_results
        -             )
        -             next!(progress)
        -         end
        -     else
        -         # Sequential evaluation
        -         for combination in param_combinations
        -             params = Dict(zip(param_names, combination))
        -             score, cv_scores = evaluate_params(params, data, optimizer.config, targets)
        -             
        -             best_score = update_best_params!(
        -                 params, score, cv_scores,
        -                 best_params, best_score, best_cv_scores,
        -                 optimization_history, all_results
        -             )
        -             
        -             next!(progress)
        -             
        -             if optimizer.config.verbose && length(optimization_history) % 10 == 0
        -                 Logger.@log_info("Current best score: $best_score")
        -             end
        -         end
        -     end
        -     
        -     training_time = time() - start_time
        -     
        -     Logger.@log_info("Grid Search completed in $(round(training_time, digits=2)) seconds")
        -     Logger.@log_info("Best score: $best_score")
        -     Logger.@log_info("Best parameters: $best_params")
        -     
        -     return OptimizationResult(
        -         best_params,
        -         best_score,
        -         all_results,
        -         optimization_history,
        -         best_cv_scores,
        -         training_time
        -     )
        - end
        - 
        - # Random Search implementation
        - function optimize_hyperparameters(
        -     optimizer::RandomSearchOptimizer,
        -     data::Dict{String,DataFrame},
        -     targets::Vector{String}
        - )
        -     start_time = time()
        -     Random.seed!(optimizer.config.seed)
        -     
        -     Logger.@log_info("Starting Random Search with $(optimizer.n_iter) iterations")
        -     
        -     # Storage for results
        -     all_results = DataFrame()
        -     optimization_history = Vector{Dict{Symbol,Any}}()
        -     best_score = -Inf
        -     best_params = Dict{Symbol,Any}()
        -     best_cv_scores = Float64[]
        -     
        -     # Progress meter
        -     progress = Progress(optimizer.n_iter, desc="Random Search: ", showspeed=true)
        -     
        -     # Generate and evaluate random parameter combinations
        -     if optimizer.config.parallel && nworkers() > 1
        -         # Generate all random combinations first
        -         param_combinations = [
        -             Dict(name => dist() for (name, dist) in optimizer.param_distributions)
        -             for _ in 1:optimizer.n_iter
        -         ]
        -         
        -         # Parallel evaluation
        -         results = pmap(param_combinations) do params
        -             score, cv_scores = evaluate_params(params, data, optimizer.config, targets)
        -             return (params=params, score=score, cv_scores=cv_scores)
        -         end
        -         
        -         for result in results
        -             best_score = update_best_params!(
        -                 result.params, result.score, result.cv_scores,
        -                 best_params, best_score, best_cv_scores,
        -                 optimization_history, all_results
        -             )
        -             next!(progress)
        -         end
        -     else
        -         # Sequential evaluation
        -         for iter in 1:optimizer.n_iter
        -             # Sample random parameters
        -             params = Dict(
        -                 name => dist() 
        -                 for (name, dist) in optimizer.param_distributions
        -             )
        -             
        -             score, cv_scores = evaluate_params(params, data, optimizer.config, targets)
        -             
        -             best_score = update_best_params!(
        -                 params, score, cv_scores,
        -                 best_params, best_score, best_cv_scores,
        -                 optimization_history, all_results
        -             )
        -             
        -             next!(progress)
        -             
        -             if optimizer.config.verbose && iter % 10 == 0
        -                 Logger.@log_info("Iteration $iter/$optimizer.n_iter - Best score: $best_score")
        -             end
        -         end
        -     end
        -     
        -     training_time = time() - start_time
        -     
        -     Logger.@log_info("Random Search completed in $(round(training_time, digits=2)) seconds")
        -     Logger.@log_info("Best score: $best_score")
        -     Logger.@log_info("Best parameters: $best_params")
        -     
        -     return OptimizationResult(
        -         best_params,
        -         best_score,
        -         all_results,
        -         optimization_history,
        -         best_cv_scores,
        -         training_time
        -     )
        - end
        - 
        - # Bayesian Optimization implementation
        - function optimize_hyperparameters(
        -     optimizer::BayesianOptimizer,
        -     data::Dict{String,DataFrame},
        -     targets::Vector{String}
        - )
        -     start_time = time()
        -     Random.seed!(optimizer.config.seed)
        -     
        -     Logger.@log_info("Starting Bayesian Optimization with $(optimizer.n_iter) iterations")
        -     
        -     # Storage for results
        -     all_results = DataFrame()
        -     optimization_history = Vector{Dict{Symbol,Any}}()
        -     best_score = -Inf
        -     best_params = Dict{Symbol,Any}()
        -     best_cv_scores = Float64[]
        -     
        -     # Observed points and scores for Gaussian Process
        -     observed_params = Vector{Vector{Float64}}()
        -     observed_scores = Float64[]
        -     
        -     # Parameter names and bounds
        -     param_names = collect(keys(optimizer.param_bounds))
        -     param_ranges = [optimizer.param_bounds[name] for name in param_names]
        -     
        -     # Progress meter
        -     total_iters = optimizer.n_initial + optimizer.n_iter
        -     progress = Progress(total_iters, desc="Bayesian Optimization: ", showspeed=true)
        -     
        -     # Initial random sampling phase
        -     for i in 1:optimizer.n_initial
        -         # Sample random parameters within bounds
        -         param_vector = [
        -             rand() * (bounds[2] - bounds[1]) + bounds[1]
        -             for bounds in param_ranges
        -         ]
        -         
        -         params = Dict(zip(param_names, param_vector))
        -         score, cv_scores = evaluate_params(params, data, optimizer.config, targets)
        -         
        -         push!(observed_params, param_vector)
        -         push!(observed_scores, score)
        -         
        -         best_score = update_best_params!(
        -             params, score, cv_scores,
        -             best_params, best_score, best_cv_scores,
        -             optimization_history, all_results
        -         )
        -         
        -         next!(progress)
        -         
        -         if optimizer.config.verbose
        -             Logger.@log_info("Initial sampling $i/$(optimizer.n_initial) - Score: $score")
        -         end
        -     end
        -     
        -     # Bayesian optimization phase
        -     for iter in 1:optimizer.n_iter
        -         # Find next point to evaluate using acquisition function
        -         next_params = select_next_point(
        -             observed_params,
        -             observed_scores,
        -             param_ranges,
        -             optimizer.acquisition_function
        -         )
        -         
        -         params = Dict(zip(param_names, next_params))
        -         score, cv_scores = evaluate_params(params, data, optimizer.config, targets)
        -         
        -         push!(observed_params, next_params)
        -         push!(observed_scores, score)
        -         
        -         best_score = update_best_params!(
        -             params, score, cv_scores,
        -             best_params, best_score, best_cv_scores,
        -             optimization_history, all_results
        -         )
        -         
        -         next!(progress)
        -         
        -         if optimizer.config.verbose && iter % 5 == 0
        -             Logger.@log_info("Iteration $iter/$(optimizer.n_iter) - Best score: $best_score")
        -         end
        -     end
        -     
        -     training_time = time() - start_time
        -     
        -     Logger.@log_info("Bayesian Optimization completed in $(round(training_time, digits=2)) seconds")
        -     Logger.@log_info("Best score: $best_score")
        -     Logger.@log_info("Best parameters: $best_params")
        -     
        -     return OptimizationResult(
        -         best_params,
        -         best_score,
        -         all_results,
        -         optimization_history,
        -         best_cv_scores,
        -         training_time
        -     )
        - end
        - 
        - # Gaussian Process implementation for Bayesian optimization
        - struct SimpleGaussianProcess
        2     kernel_func::Function
        -     noise::Float64
        -     length_scale::Float64
        - end
        - 
        4 function SimpleGaussianProcess(; noise=1e-6, length_scale=1.0)
        -     # RBF (Radial Basis Function) kernel
       16     kernel_func = (x1, x2, l) -> exp(-0.5 * sum(((x1 .- x2) ./ l).^2))
        2     return SimpleGaussianProcess(kernel_func, noise, length_scale)
        - end
        - 
        - # Compute kernel matrix
        2 function compute_kernel_matrix(gp::SimpleGaussianProcess, X::Vector{Vector{Float64}})
        2     n = length(X)
        8     K = zeros(n, n)
        2     for i in 1:n
        4         for j in 1:n
        8             K[i, j] = gp.kernel_func(X[i], X[j], gp.length_scale)
       12         end
        4         K[i, i] += gp.noise  # Add noise to diagonal
        6     end
        2     return K
        - end
        - 
        - # GP prediction with proper mean and variance
        2 function gp_predict(gp::SimpleGaussianProcess, X_train::Vector{Vector{Float64}}, 
        -                    y_train::Vector{Float64}, X_test::Vector{Float64})
        2     if isempty(X_train)
        0         return 0.0, 1.0
        -     end
        -     
        -     # Compute kernel matrices
        2     K = compute_kernel_matrix(gp, X_train)
        2     k_star = [gp.kernel_func(X_test, x_train, gp.length_scale) for x_train in X_train]
        2     k_star_star = gp.kernel_func(X_test, X_test, gp.length_scale) + gp.noise
        -     
        -     # Compute mean and variance using Cholesky decomposition for numerical stability
        2     try
        2         L = cholesky(Symmetric(K)).L
        4         alpha = L' \ (L \ y_train)
        2         mean = dot(k_star, alpha)
        -         
        2         v = L \ k_star
        2         variance = k_star_star - dot(v, v)
        2         variance = max(variance, 1e-6)  # Ensure positive variance
        -         
        2         return mean, variance
        -     catch e
        -         # Fallback to simple inverse if Cholesky fails
        0         K_inv = inv(K + I * 1e-4)  # Add regularization
        0         mean = dot(k_star, K_inv * y_train)
        0         variance = k_star_star - dot(k_star, K_inv * k_star)
        0         variance = max(variance, 1e-6)
        0         return mean, variance
        -     end
        - end
        - 
        - # Select next point using acquisition function with proper GP
        - function select_next_point(
        -     observed_params::Vector{Vector{Float64}},
        -     observed_scores::Vector{Float64},
        -     param_ranges::Vector{Tuple{Float64,Float64}},
        -     acquisition_function::Symbol
        - )
        -     # Initialize Gaussian Process
        -     gp = SimpleGaussianProcess(noise=1e-6, length_scale=1.0)
        -     
        -     # Normalize parameters to [0, 1] for better GP performance
        -     normalized_params = normalize_params(observed_params, param_ranges)
        -     
        -     # Standardize scores for better GP performance
        -     mean_score = isempty(observed_scores) ? 0.0 : mean(observed_scores)
        -     std_score = isempty(observed_scores) ? 1.0 : std(observed_scores)
        -     std_score = std_score == 0 ? 1.0 : std_score
        -     normalized_scores = (observed_scores .- mean_score) ./ std_score
        -     
        -     n_candidates = 2000  # Increased for better optimization
        -     best_candidate = nothing
        -     best_acquisition = -Inf
        -     
        -     # Latin Hypercube Sampling for better coverage
        -     candidates = generate_lhs_samples(n_candidates, param_ranges)
        -     
        -     for candidate in candidates
        -         # Normalize candidate
        -         norm_candidate = normalize_param(candidate, param_ranges)
        -         
        -         # Get GP predictions
        -         mean_pred, var_pred = gp_predict(gp, normalized_params, normalized_scores, norm_candidate)
        -         
        -         # Denormalize predictions
        -         mean_pred = mean_pred * std_score + mean_score
        -         var_pred = var_pred * std_score^2
        -         
        -         # Calculate acquisition value
        -         if acquisition_function == :ei
        -             acquisition = calculate_expected_improvement_gp(
        -                 mean_pred, sqrt(var_pred),
        -                 observed_scores
        -             )
        -         elseif acquisition_function == :ucb
        -             acquisition = calculate_upper_confidence_bound_gp(
        -                 mean_pred, sqrt(var_pred),
        -                 beta=2.0
        -             )
        -         else
        -             error("Unknown acquisition function: $acquisition_function")
        -         end
        -         
        -         if acquisition > best_acquisition
        -             best_acquisition = acquisition
        -             best_candidate = candidate
        -         end
        -     end
        -     
        -     return best_candidate
        - end
        - 
        - # Latin Hypercube Sampling for better space exploration
        - function generate_lhs_samples(n_samples::Int, param_ranges::Vector{Tuple{Float64,Float64}})
        -     n_dims = length(param_ranges)
        -     samples = Vector{Vector{Float64}}()
        -     
        -     # Create Latin Hypercube grid
        -     for i in 1:n_samples
        -         sample = Float64[]
        -         for (low, high) in param_ranges
        -             # Stratified sampling within each dimension
        -             val = (i - 1 + rand()) / n_samples
        -             push!(sample, low + val * (high - low))
        -         end
        -         push!(samples, sample)
        -     end
        -     
        -     # Shuffle within each dimension for better randomization
        -     for dim in 1:n_dims
        -         perm = randperm(n_samples)
        -         for i in 1:n_samples
        -             samples[i][dim], samples[perm[i]][dim] = samples[perm[i]][dim], samples[i][dim]
        -         end
        -     end
        -     
        -     return samples
        - end
        - 
        - # Normalize parameters to [0, 1] range
        - function normalize_params(params::Vector{Vector{Float64}}, ranges::Vector{Tuple{Float64,Float64}})
        -     return [normalize_param(p, ranges) for p in params]
        - end
        - 
        - function normalize_param(param::Vector{Float64}, ranges::Vector{Tuple{Float64,Float64}})
        -     return [(p - r[1]) / (r[2] - r[1]) for (p, r) in zip(param, ranges)]
        - end
        - 
        - # Expected Improvement with proper Gaussian Process
        1 function calculate_expected_improvement_gp(
        -     mean::Float64,
        -     std::Float64,
        -     observed_scores::Vector{Float64}
        - )
        2     if isempty(observed_scores) || std < 1e-9
        0         return 0.0
        -     end
        -     
        1     current_best = maximum(observed_scores)
        1     z = (mean - current_best) / std
        -     
        -     # Use the cumulative distribution function (CDF) and probability density function (PDF)
        -     # of the standard normal distribution
        1     dist = Normal(0, 1)
        1     ei = std * (z * cdf(dist, z) + pdf(dist, z))
        -     
        1     return ei
        - end
        - 
        - # Upper Confidence Bound with proper Gaussian Process
        2 function calculate_upper_confidence_bound_gp(
        -     mean::Float64,
        -     std::Float64;
        -     beta::Float64=2.0
        - )
        -     # UCB = mean + beta * std
        1     return mean + beta * std
        - end
        - 
        - # Helper function to update best parameters
        2 function update_best_params!(
        -     params::Dict{Symbol,Any},
        -     score::Float64,
        -     cv_scores::Vector{Float64},
        -     best_params::Dict{Symbol,Any},
        -     best_score::Float64,
        -     best_cv_scores::Vector{Float64},
        -     optimization_history::Vector{Dict{Symbol,Any}},
        -     all_results::DataFrame
        - )
        -     # Update history
        2     push!(optimization_history, Dict(
        -         :params => params,
        -         :score => score,
        -         :cv_scores => cv_scores,
        -         :timestamp => now()
        -     ))
        -     
        -     # Update best if improved
        -     # Find previous best score from history (excluding the current entry we just added)
        3     previous_best_score = length(optimization_history) <= 1 ? -Inf : maximum(h[:score] for h in optimization_history[1:end-1])
        -     
        4     if score > previous_best_score
        1         empty!(best_params)
        1         merge!(best_params, params)
        1         previous_best_score = score
        1         empty!(best_cv_scores)
        2         append!(best_cv_scores, cv_scores)
        -     end
        -     
        -     # Return the actual current best score (the one we should be tracking)
        3     current_best_score = length(optimization_history) == 1 ? score : max(previous_best_score, score)
        -     
        -     # Add to results DataFrame
        4     result_row = DataFrame(
        -         score = score,
        -         cv_mean = mean(cv_scores),
        -         cv_std = std(cv_scores)
        -     )
        -     
        4     for (key, value) in params
        4         result_row[!, key] = [value]
        6     end
        -     
        2     append!(all_results, result_row)
        -     
        2     return current_best_score
        - end
        - 
        - # Get best parameters from optimization result
        1 function get_best_params(result::OptimizationResult)
        1     return result.best_params
        - end
        - 
        - # Save optimization results to file
        1 function save_optimization_results(result::OptimizationResult, filepath::String)
        1     results_dict = Dict(
        -         "best_params" => result.best_params,
        -         "best_score" => result.best_score,
        -         "cv_scores" => result.cross_validation_scores,
        -         "training_time" => result.training_time,
        -         "optimization_history" => result.optimization_history
        -     )
        -     
        1     open(filepath, "w") do io
        1         JSON3.write(io, results_dict)
        -     end
        -     
        1     Logger.@log_info("Optimization results saved to $filepath")
        - end
        - 
        - # Load optimization results from file
        1 function load_optimization_results(filepath::String)
        1     results_dict = JSON3.read(read(filepath, String), Dict{String,Any})
        -     
        -     # Convert string keys back to symbols in history
        1     history = Vector{Dict{Symbol,Any}}()
        1     for hist_item in results_dict["optimization_history"]
        1         hist_dict = Dict{Symbol,Any}()
        1         for (k, v) in hist_item
        2             if k == "params" && v isa Dict
        -                 # Convert params dict keys to symbols
        1                 hist_dict[Symbol(k)] = Dict(Symbol(pk) => pv for (pk, pv) in v)
        -             else
        1                 hist_dict[Symbol(k)] = v
        -             end
        2         end
        1         push!(history, hist_dict)
        1     end
        -     
        1     return OptimizationResult(
        -         Dict(Symbol(k) => v for (k, v) in results_dict["best_params"]),
        -         results_dict["best_score"],
        -         DataFrame(),  # All results not saved for space
        -         history,
        -         results_dict["cv_scores"],
        -         results_dict["training_time"]
        -     )
        - end
        - 
        - # Wrapper functions for test compatibility - these provide the expected interface
        1 function calculate_expected_improvement(
        -     candidate::Vector{Float64},
        -     observed_params::Vector{Vector{Float64}},
        -     observed_scores::Vector{Float64}
        - )
        -     # Initialize GP for prediction
        1     gp = SimpleGaussianProcess(noise=1e-6, length_scale=1.0)
        -     
        -     # Standardize scores
        1     if isempty(observed_scores)
        0         return 0.0
        -     end
        -     
        1     mean_score = mean(observed_scores)
        2     std_score = std(observed_scores)
        1     std_score = std_score == 0 ? 1.0 : std_score
        2     normalized_scores = (observed_scores .- mean_score) ./ std_score
        -     
        -     # Get GP predictions
        1     mean_pred, var_pred = gp_predict(gp, observed_params, normalized_scores, candidate)
        -     
        -     # Denormalize predictions
        1     mean_pred = mean_pred * std_score + mean_score
        1     var_pred = var_pred * std_score^2
        -     
        -     # Calculate Expected Improvement
        1     return calculate_expected_improvement_gp(mean_pred, sqrt(var_pred), observed_scores)
        - end
        - 
        2 function calculate_upper_confidence_bound(
        -     candidate::Vector{Float64},
        -     observed_params::Vector{Vector{Float64}},
        -     observed_scores::Vector{Float64};
        -     beta::Float64 = 2.0
        - )
        -     # Initialize GP for prediction
        1     gp = SimpleGaussianProcess(noise=1e-6, length_scale=1.0)
        -     
        -     # Standardize scores
        1     if isempty(observed_scores)
        0         return 0.0
        -     end
        -     
        1     mean_score = mean(observed_scores)
        2     std_score = std(observed_scores)
        1     std_score = std_score == 0 ? 1.0 : std_score
        2     normalized_scores = (observed_scores .- mean_score) ./ std_score
        -     
        -     # Get GP predictions
        1     mean_pred, var_pred = gp_predict(gp, observed_params, normalized_scores, candidate)
        -     
        -     # Denormalize predictions
        1     mean_pred = mean_pred * std_score + mean_score
        1     var_pred = var_pred * std_score^2
        -     
        -     # Calculate Upper Confidence Bound
        1     return calculate_upper_confidence_bound_gp(mean_pred, sqrt(var_pred), beta=beta)
        - end
        - 
        - end # module HyperOpt
